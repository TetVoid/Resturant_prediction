Подготовка данны:
	загружаем данные из train.csv, data.csv и restaurants.csv далее объеденяем загруженные данные
	в одну таблицу. train.csv и data.csv объединяем в одну таблицу по столбцам "City", "Date", "IsHoliday"
	с получившейся таблице объеденяем "restaurants.csv" по столбцу "City" 
	из получившейся таблицы удаляем столбец "Date"

Процесс выбора архитектуры и настройки модели(используемая метрика R2_score): 
	линейная множественная регрессия 0.0853
	метод ближайших к соседий 0.0592
	метод случайных деревьев 0.7742
		качественные предикторы(ресторан, город) кодируются порядково

	линейная множественная регрессия 0.6424
	метод ближайших к соседий 0.4149
	метод случайных деревьев 0.9107
		качественные предикторы(ресторан, город) кодируются через индикаторные переменные

	линейная множественная регрессия 0.6368
	метод ближайших к соседий 0.4235
	метод случайных деревьев 0.9190
		добавлены полиномы для каждого предиктора (кроме качественных предикторов)degree=2
		принято решение больше не тестировать метод ближайших к соседей в виду отсудствия значительного роста его точности
	
 	линейная множественная регрессия 0.8997
	метод случайных деревьев 0.9140
		полиномизация всех данных (время обучения значительно увеличелось количество предикторов равно 7875)
		так же были вырезанны выбросы по столбцам: "Temperature", "Fuel_Price", "Unemployment" 
		выбросы определялись по формуле все значения выше 75% квартиля + стандартное отклонение наблюдений
	
	линейная множественная регрессия 0.9092
	метод случайных деревьев 0.9237
		вместо удаления выбросов их значения приводиться в верхней границе: 75% квартиля + стандартное отклонение наблюдений

Выводы:
	Хоть результаты множественной линейной регрссии и удалось довести до значений сопастовимых с методом случайнных деревьев,
	всё же метод случайных деревьев показывает себя с лучшей стороны. В рамках тестов был сделан вывод что полиномизация параметров
	не влияет на качество работы метода случайных деревьев.
